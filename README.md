This repository has Recommender System code related to 1) Collaborative Filtering (via matrix factorization) and 2) Retrieval & Ranking (with a neural net). Both techniques are used to suggest items to show to users and both techniques involve taking a dot product of a user embedding and an item embeddings for scoring. (high score => high ranking for recommendations)

The recommender.ipynb comes from the following recommender system tutorial on collaborative filtering via matrix factorization of a utility matrix:
https://www.youtube.com/watch?v=XfAe-HLysOM
https://github.com/topspinj/tmls-2020-recommender-workshop

This notebook has additional notes, modifications, and code I added as I went through the tutorial. Some supplemental movie ratings were also added to the ratings file dataset.

This collaborative filtering includes item-to-item and user-to-user-to-item movie recommendations via vector similarity analysis. This type of recommender system may tend to fall under batch recommendation systems, where the model is trained every day/week/month in a large batch offline and recommended items are saved to a database and served to the user at next login. This might make sense for feature-length movie site like Tubi (see: https://tubitv.com/movies/100015091/digital-physics ) where you want to recommend movies that span the past six decades. One downside for using this on something like Facebook or Youtube is that it may not capture temporal features or aspects that capture/emphasize/weight the new, viral, or relevant to what you just engaged with. Also, this approach suffers from the cold start problem for new items or users that don't have a interaction history. A final downside is that factoring a large matrix using Alternating Least Squares of Singular Value Decomposition may be computationally expensive and not always easy for large matrices. 

It's worth noting that is possible to have variations of this matrix factorization approach. For instance, if you wanted to add a time aspect to the recommender, you could use the last three movies the user clicked on (or rated favorably) as the query points for finding similar items/movies/vectors to recommend the user next. Another benefit collaborative filtering on user interactions/ratings/utility has is that we don't need to know anything about the users or items to get their embeddings (of length k via factoring mXn ~= dot(mxk, nxk.T)) and make recommendations. No feature engineering needed! This is different than content-based filtering where the user and item embeddings originate from features related to users (e.g. age, location, etc.) and items (e.g. likes, genre, Bag of Words text embedding, etc.). But even in content-based filtering, the embeddings are also shaped by the interaction data during training because the rating/interaction data in the utility/interaction matrix are the ground truths.

We're also going to explore Neural Collaborative Filtering using a Two (ReLu) Tower (Neural Net) to get embeddings to dot product. This is a retrieval & ranking approach that offers a couple advantages. First, it handles the cold start problem better. We don't need to wait for interactions to get an embedding for new users and items. Secondly, it doesn't require matrix factorization which can be difficult and computationally expensive for large matrices. 

There is still more to explore here on different aspects of collaborative filtering via matrix factorization. Even though collaborative filtering is simpler in theory than the Retrieval & Ranking recommendation approach, there is still a lot to think about. Some things to explore include distance metric considerations (e.g. Euclidean, Cosine Similarity, etc.), pre-processing (e.g. ways to handle missing values in sparse vectors (e.g. imputing average, ignoring, treating as 0, treating as infinity), post-processing code (e.g. filter movies that were seen already, adding diversity of content, etc.), composite utility values (e.g. utility_matrix[user_i][movie_j] = user_i_hours_watched_movie_j + user_i_rating_movie_j + 2 * user_i_likes_movie_j + 5 * user_i_shared_movie_j + 2 * user_i_comment_on_movie_j), normalizing vectors in the matrix (e.g. user_i_rating_movie_j/avg(user_i_rating)), etc. 

In the theory folder, we explore several rabbit holes related to this project. One thing we explore is gradient descent instead of Singular Value Decomposition for matrix factorization. (We'd still like to code up Alternating Least Squares.) To expand on above, matrix factorization of the 600 users X 10_000 movies is one way to help derive user and item embeddings for representing users and items in a compressed way. You could imagine a vector embedding of just 20 floats instead of sparse vector of 10_000 representing a user. Similarly, you might have a movie embedding that is just 20 floats instead of the length of the user set, 600. Then the dot product of user i and movie j embeddings would hopefully give you a number that isn't too far off what you see in the utility/interaction matrix. Using these embedding for similarity search might not only save memory space compared the 600 x 10_000 utility matrix, but you also no longer have to worry about how your distance metric (for similarity search) is going to handle missing components in your sparse vector in an unbiased way, as user and item vectors will no longer be sparse.

![SVD or Gradient Descent Utility Matrix factorization](./img/utility_matrix_factorization.png)

We are also going to look at real-time recommendations using a Retrieval & Ranking architecture. In this approach, we go from billions of possible item candidates
down to hundreds in an Approximate Nearest Neighbor (lower time complexity than actual KNN) retrieval step, and then run each of those through a neural network predictor for ranking purposes. The ground truth is still the same; it's the values of the sparse utility/interaction matrix. This is why it is called neural collaborative filtering. We won't have the cold start problem where we need to wait for the next collaborative filtering training to get embeddings for new items or users that come in our last matrix factorization. How? We'll use traditional feature engineering for the user features and item features rather than just getting an embeddings from interactions. We use a two (ReLu) tower (neural net) architecture for training a recommendation predictor/ranker. A potential benefit of a two tower approach (over a single ReLu tower neural net) is that you are bringing the enriched user and item embeddings together in a dot product at the end, right before prediction and then the subsequent start of back propagation. This approach makes the neural net approach to getting user and item embeddings (at the top of our towers) analogous to matrix factorization approach to embeddings. In both situations, we want dot(user_embedding_i, item_embedding_j) to be high when there is a good match in our interactions/utility matrix.

The dot product of the two tower approach should be followed by a sigmoid for squashing between 0 and 1 if engagement/no-engagement is your ground truth (binary classifier), or a ReLu or no activation at all if your ground truth is a composite utility metric that ranges from say 0 to 10 (regression). In terms of the cost function to optimize, a binary cross entropy (aka a log loss) for binary classification could be used or a Mean Square Error for regression could be used. In terms of optimizers, you might want to try SGD, Adam, etc. 

For these tower models, you could add user and item embeddings in as features (see two_tower_predictor_with_embedding_features.py), although I think we can safely omit these embeddings altogether and just use traditional content-based features since the interactions we train the two towers on also come from the ground truth utility/interaction matrix. 

One thing to note: for a social media app, the utility/interaction matrix we train on should have positive engagement and negative engagement from only movies that have already been recommend and seen, not just rated movies. There are other variations to think through related to the specifics of the product/website recommender.

Post-processing is always needed whether we are doing collaborative filtering through matrix factorization or retrieval & ranking via neural collaborative filtering. We can filter for inappropriate content, already watched content, or shuffle the ranking results for diversity or other reasons such as advertising or promoting.

An additional consideration which we already touched on is putting the model into production. Can we serve the recommender so that it has low latency (for recommendations), scalability to many users and items, and retraining pipelines? Some of these considerations seem to favor the two tower approach to the collaborative filtering matrix factorization approach.

![architecture image](./img/retrieval_ranking.png)
![two tower](./img/two_tower.png)

https://www.youtube.com/watch?v=9vBRjGgdyTY

An additional note on the evolution of retrieval: Early on, retrieval dealt with finding exact matches in an efficient way (e.g. is the sting "digital" in a text corpus?). With collaborative filtering, we were no longer interested in exact matches, but close matches in a vector space. Collaborative filtering was purely based on interaction data. Matrix factorization then became popular using (SVD or Alternating Least Squares) for getting embeddings. Now with the rise of deep learning, we have Word2Vec and neural collaborative filtering like our two-tower approach for getting embeddings that can capture some semantic meaning.



